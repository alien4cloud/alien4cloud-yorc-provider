tosca_definitions_version: ${alien4cloud.dsl.version}

template_name: yorc-hostspool-types
template_author: Yorc
template_version: ${yorc.hp.types.version}


# Note: If we reference a dependency using project.version then this component should
#       be versioned as project.version too otherwise we will have issues on snapshots
#       versions (our type not snapshot referencing a snapshot version)
imports:
  - tosca-normative-types:${tosca.normative.types.version}
  - yorc-types:${yorc.types.version}

node_types:
  yorc.nodes.hostspool.Compute:
    derived_from: yorc.nodes.Compute
    properties:
      shareable:
        type: boolean
        description: Specify if the compute can be shared.
        required: false
        default: false
      filters:
        type: list
        entry_schema:
          type: string
        required: false
    attributes:
      hostname:
        type: string
        description: The hostname as known in the hosts pool
    capabilities:
      endpoint:
        type: yorc.capabilities.Endpoint.ProvisioningAdmin
        properties:
          credentials:
            user: "not significant, will be set by Yorc itself"
      host:
        type: yorc.capabilities.hostspool.Container


policy_types:
  yorc.policies.hostspool.Placement:
    abstract: true
    derived_from: tosca.policies.Placement
    description: The yorc hostpool TOSCA Policy Placement.

  yorc.policies.hostspool.WeightBalancedPlacement:
    derived_from: yorc.policies.hostspool.Placement
    description: >
      The yorc hostpool TOSCA Policy placement which allows to allocate a host with a weight-balanced algorithm.
      It means the host the less allocated will be elect preferentially.
    metadata:
      # pluginId:pluginBean:phase
      a4c_policy_impl: alien4cloud-yorc-provider:yorc-hostspool-placement-modifier:post-node-match
    targets: [ tosca.nodes.Compute ]

  yorc.policies.hostspool.BinPackingPlacement:
    derived_from: yorc.policies.hostspool.Placement
    description: >
      The yorc hostpool TOSCA Policy placement which allows to allocate a host with a bin packing algorithm.
      It means the host the more allocated will be elect preferentially.
    metadata:
      # pluginId:pluginBean:phase
      a4c_policy_impl: alien4cloud-yorc-provider:yorc-hostspool-placement-modifier:post-node-match
    targets: [ tosca.nodes.Compute ]

capability_types:
  yorc.capabilities.hostspool.Container:
    derived_from: tosca.capabilities.Container
    properties:
      resources:
        type: list
        description: >
          A list of generic resources that the container must provide.
        entry_schema:
          type: yorc.datatypes.hostspool.GenericResource
        required: false

data_types:
  yorc.datatypes.hostspool.GenericResource:
    derived_from: tosca.datatypes.Root
    properties:
      name:
        type: string
        required: true
        description: >
          The name of the generic resource. Can be "gpu" and must be bound to host labels as: host.resource.gpu by instance.
      ids:
        type: string
        required: false
        description: >
          Comma-separated list of required generic resource ID's. Either ids or number must be filled to define the resource need.
      number:
        type: integer
        required: false
        description: >
          The number of generic resource required.  Either ids or number must be filled to define the resource need.
      no_consumable:
        type: boolean
        required: false
        default: false
        description: >
          By default, the generic resource is consumable. It means a resource can be only used by a single compute.
          If you allow the resource to be shared by multiple computes, you have to set this property to true.

